Traceback (most recent call last):
  File "main.py", line 43, in <module>
    wrapper(parameters)
  File "main.py", line 24, in wrapper
    initializer(frame_type, model_type, loss_type, audio_path, model_name)
  File "/gpfs/home/egutierrez/ddsp_textures/training/initializer.py", line 75, in initializer
    dataset.compute_dataset()
  File "/gpfs/home/egutierrez/ddsp_textures/dataset/dataset_maker.py", line 79, in compute_dataset
    feature_0 = torchaudio.functional.spectral_centroid(segment_rate_shifted_tensor, self.sampling_rate, 0, torch.hamming_window(self.frame_size), self.frame_size, self.frame_size, self.frame_size)[0]
  File "/home/egutierrez/.local/lib/python3.8/site-packages/torchaudio/functional/functional.py", line 1283, in spectral_centroid
    specgram = spectrogram(
  File "/home/egutierrez/.local/lib/python3.8/site-packages/torchaudio/functional/functional.py", line 126, in spectrogram
    spec_f = torch.stft(
  File "/home/egutierrez/.local/lib/python3.8/site-packages/torch/functional.py", line 663, in stft
    input = F.pad(input.view(extended_shape), [pad, pad], pad_mode)
  File "/home/egutierrez/.local/lib/python3.8/site-packages/torch/nn/functional.py", line 4522, in pad
    return torch._C._nn.pad(input, pad, mode, value)
RuntimeError: Argument #4: Padding size should be less than the corresponding input dimension, but got: padding (32768, 32768) at dimension 2 of input [1, 1, 2292]
