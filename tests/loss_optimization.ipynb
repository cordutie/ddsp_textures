{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddsp_textures.dataset.makers    import read_wavs_from_folder\n",
    "from ddsp_textures.auxiliar.features import *\n",
    "\n",
    "audio_path    = ...\n",
    "sampling_rate = ...\n",
    "frame_size    = ...\n",
    "hop_size      = ...\n",
    "audios_list   = read_wavs_from_folder(audio_path, sampling_rate)\n",
    "data          = []\n",
    "for audio in audios_list:\n",
    "    size = len(audio)\n",
    "    number_of_segments = (size - frame_size) // hop_size\n",
    "    print(f\"Number of segments: {number_of_segments}\")\n",
    "    for i in range(number_of_segments):\n",
    "        segment = audio[i * hop_size : i * hop_size + frame_size]\n",
    "        segment = audio_improver(segment, sampling_rate, 4)\n",
    "        segment = signal_normalizer(segment)\n",
    "        data.append([segment, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from ddsp_textures.loss.functions import *\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Define constants (replace these with actual values)\n",
    "N_filter_bank = ...\n",
    "M_filter_bank = ...\n",
    "erb_bank      = ...\n",
    "log_bank      = ...\n",
    "downsampler   = ...\n",
    "\n",
    "# Convert data into tensors if necessary\n",
    "signals    = [item[0] for item in data]\n",
    "categories = [item[1] for item in data]\n",
    "\n",
    "# Create a DataLoader for batching\n",
    "batch_size = 16  # Choose a batch size based on available memory\n",
    "dataset    = TensorDataset(torch.tensor(signals), torch.tensor(categories))\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize alpha with random values\n",
    "alpha = torch.randn(5, requires_grad=True)  # Start with random values\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam([alpha], lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        batch_signals, batch_categories = batch\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Apply softmax to ensure sum constraint on alpha\n",
    "        normalized_alpha = torch.softmax(alpha, dim=0)\n",
    "\n",
    "        # Calculate loss within the batch\n",
    "        batch_loss = 0\n",
    "        batch_size = len(batch_signals)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            for j in range(i + 1, batch_size):\n",
    "                signal_1, category_1 = batch_signals[i], batch_categories[i]\n",
    "                signal_2, category_2 = batch_signals[j], batch_categories[j]\n",
    "\n",
    "                # Calculate loss using batch_statistics_loss\n",
    "                if category_1 == category_2:\n",
    "                    # Minimize distance for same class\n",
    "                    batch_loss += statistics_loss(\n",
    "                        signal_1, signal_2, N_filter_bank, M_filter_bank,\n",
    "                        erb_bank, log_bank, downsampler, normalized_alpha\n",
    "                    )\n",
    "                else:\n",
    "                    # Maximize distance for different classes\n",
    "                    batch_loss -= statistics_loss(\n",
    "                        signal_1, signal_2, N_filter_bank, M_filter_bank,\n",
    "                        erb_bank, log_bank, downsampler, normalized_alpha\n",
    "                    )\n",
    "        \n",
    "        # Backpropagate and optimize\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track total loss for reporting\n",
    "        total_loss += batch_loss.item()\n",
    "    \n",
    "    # Optionally print progress\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {total_loss}\")\n",
    "\n",
    "# Final optimized parameters\n",
    "final_alpha = torch.softmax(alpha, dim=0).detach()\n",
    "print(\"Optimized Parameters:\", final_alpha)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
